{"cells":[{"cell_type":"markdown","metadata":{"id":"GEmVUJk1Or77"},"source":["## About GANs\n","\n","Since GANs are quite advanced, diving deeply into the technical details would be out of scope for us. This specific implementation will be a **deep convolutional GAN, or DCGAN**: a GAN where the generator and discriminator are deep convnets.\n","In particular, it leverages a `Conv2DTranspose` layer for image upsampling in the generator.\n","\n","We will train our GAN on images from **CIFAR10**, a dataset of 50,000 32x32 RGB images belong to 10 classes (5,000 images per class). To make things even easier, we will only use images belonging to the class \"frog\".\n","\n","Schematically, our GAN looks like this:\n","\n","- A generator network maps vectors of shape (latent_dim,) to images of shape (32, 32, 3).\n","- A discriminator network maps images of shape (32, 32, 3) to a binary score estimating the probability that the image is real.\n","- A gan network chains the generator and the discriminator together: `gan(x) = discriminator(generator(x))`. Thus this gan network maps latent space vectors to the discriminator's assessment of the realism of these latent vectors as decoded by the generator.\n","- We train the discriminator using examples of real and fake images along with \"real\"/\"fake\" labels, as we would train any regular image classification model.\n","- To train the generator, we use the gradients of the generator's weights with regard to the loss of the gan model. This means that, at every step, we move the weights of the generator in a direction that will make the discriminator more likely to classify as \"real\" the images decoded by the generator. I.e. we train the generator to fool the discriminator."]},{"cell_type":"markdown","metadata":{"id":"dHFZrRSsOr8F"},"source":["### A bag of tricks\n","\n","Here are a few of the tricks that we leverage in our own implementation of a GAN generator and discriminator below.\n","\n","- We use `tanh` as the last activation in the generator, instead of `sigmoid`, which would be more commonly found in other types of models.\n","- We sample points from the latent space using a normal distribution (Gaussian distribution), not a uniform distribution.\n","- Stochasticity is good to induce robustness. Since GAN training results in a dynamic equilibrium, GANs are likely to get \"stuck\" in all sorts of ways. Introducing randomness during training helps prevent this. We introduce randomness in two ways:\n","    1. we use dropout in the discriminator;\n","    2. we add some random noise to the labels for the discriminator.\n","- Sparse gradients can hinder GAN training. In deep learning, sparsity is often a desirable property, but not in GANs. There are two things that can induce gradient sparsity:\n","    1. max pooling operations;\n","    2. ReLU activations. Instead of max pooling, we recommend using strided convolutions for downsampling, and we recommend using a LeakyReLU layer instead of a ReLU activation. It is similar to ReLU but it relaxes sparsity constraints by allowing small negative activation values.\n","- In generated images, it is common to see \"checkerboard artifacts\" caused by unequal coverage of the pixel space in the generator. To fix this, we use a kernel size that is divisible by the stride size, whenever we use a strided Conv2DTranpose or Conv2D in both the generator and discriminator."]},{"cell_type":"markdown","metadata":{"id":"buklsIxfOr8H"},"source":["### The generator\n","\n","First, we develop a `generator` model, which turns a vector (from the latent space -- during training it will sampled at random) into a candidate image. One of the many issues that commonly arise with GANs is that the generator gets stuck with generated images that look like noise. A possible solution is to use dropout on both the discriminator and generator."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6088,"status":"ok","timestamp":1707192565253,"user":{"displayName":"kumod kumar gupta","userId":"07478795497397479293"},"user_tz":-330},"id":"EKZr-Lu8Or8I","outputId":"d5a47a29-a76e-4a15-d7f2-8a929731cba3","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 32)]              0         \n","                                                                 \n"," dense (Dense)               (None, 32768)             1081344   \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 32768)             0         \n","                                                                 \n"," reshape (Reshape)           (None, 16, 16, 128)       0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 16, 16, 256)       819456    \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 256)       0         \n","                                                                 \n"," conv2d_transpose (Conv2DTr  (None, 32, 32, 256)       1048832   \n"," anspose)                                                        \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 256)       0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 32, 32, 256)       1638656   \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 256)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 32, 32, 256)       1638656   \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 32, 32, 3)         37635     \n","                                                                 \n","=================================================================\n","Total params: 6264579 (23.90 MB)\n","Trainable params: 6264579 (23.90 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","from tensorflow.keras import layers\n","#the latent dimension in GANs represents the space from which the generator generates synthetic data, and its choice influences the diversity and quality of the generated samples as well as the training dynamics of the GAN.\n","latent_dim = 32     # latent dimension\n","height = 32\n","width = 32\n","channels = 3\n","\n","generator_input = tf.keras.Input(shape=(latent_dim,))\n","\n","# First, transform the input into a 16x16 128-channels feature map\n","x = layers.Dense(128 * 16 * 16)(generator_input)\n","x = layers.LeakyReLU()(x)\n","x = layers.Reshape((16, 16, 128))(x)\n","\n","# Then, add a convolution layer\n","x = layers.Conv2D(256, 5, padding='same')(x)   # 256 Neurons, 5 Kernal shape\n","x = layers.LeakyReLU()(x)\n","\n","# Upsample to 32x32\n","x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","\n","# Few more conv layers\n","x = layers.Conv2D(256, 5, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(256, 5, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","\n","# Produce a 32x32 1-channel feature map\n","x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)   # Hyperbolic Tangent, TanH\n","generator = tf.keras.models.Model(generator_input, x)\n","\n","generator.summary()"]},{"cell_type":"markdown","metadata":{"id":"yVwDTLRkOr8M"},"source":["### The discriminator\n","\n","Then, we develop a `discriminator` model, that takes as input a candidate image (real or synthetic) and classifies it into one of two classes, either \"generated image\" or \"real image that comes from the training set\"."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1707192565254,"user":{"displayName":"kumod kumar gupta","userId":"07478795497397479293"},"user_tz":-330},"id":"p7u9MNpkOr8N","outputId":"573c3d09-2e0f-4055-9e50-4a6a08112f1a","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"]}],"source":["discriminator_input = layers.Input(shape=(height, width, channels))\n","x = layers.Conv2D(128, 3)(discriminator_input)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(128, 4, strides=2)(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(128, 4, strides=2)(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(128, 4, strides=2)(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Flatten()(x)\n","\n","# One dropout layer - important trick!\n","x = layers.Dropout(0.4)(x)   #Dropout in CNNs helps prevent overfitting by randomly deactivating a proportion of neurons during training. This encourages the network to learn more robust features and reduces its reliance on specific neurons, leading to better generalization and robustness on unseen data.\n","\n","# Classification layer\n","x = layers.Dense(1, activation='sigmoid')(x)\n","\n","discriminator = tf.keras.models.Model(discriminator_input, x)\n","\n","#discriminator.summary()\n","\n","# To stabilize training, we use learning rate decay\n","# and gradient clipping (by value) in the optimizer.\n","#discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate==0.0008, clipvalue= 1.0, decay=1e-8)  # gradient value clipping ( clipvalue -1 to1 ).         # lr: learning rate\n","discriminator_optimizer = tf.keras.optimizers.RMSprop(lr=0.0008,clipvalue= 1.0)  # gradient value clipping ( clipvalue -1 to1 ).         # lr: learning rate\n","discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"]},{"cell_type":"markdown","metadata":{"id":"qZsQkenKOr8P"},"source":["### The adversarial network\n","\n","Finally, we setup the GAN, which chains the generator and the discriminator. This is the model that, when trained, will move the generator in a direction that improves its ability to fool the discriminator. This model turns latent space points into a classification decision, \"fake\" or \"real\", and it is meant to be trained with labels that are always \"these are real images\". So training `gan` will updates the weights of `generator` in a way that makes `discriminator` more likely to predict \"real\" when looking at fake images. Very importantly, we set the discriminator to be frozen during training (non-trainable): its weights will not be updated when training `gan`. If the discriminator weights could be updated during this process, then we would be training the discriminator to always predict \"real\", which is not what we want!"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1707192565254,"user":{"displayName":"kumod kumar gupta","userId":"07478795497397479293"},"user_tz":-330},"id":"hnMy-VIkOr8Q","outputId":"52dd5611-fb3c-49ab-ec65-ae9a6e5d06bb","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"]}],"source":["# Set discriminator weights to non-trainable\n","# (will only apply to the `gan` model)\n","discriminator.trainable = False\n","\n","gan_input = tf.keras.Input(shape=(latent_dim,))\n","gan_output = discriminator(generator(gan_input))\n","gan = tf.keras.models.Model(gan_input, gan_output)\n","\n","#gan_optimizer = tf.keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8) #clipvalue=1.0 means that the gradients will be clipped to have a maximum absolute value of 1.0. If the gradient value exceeds this threshold, it will be clipped to either 1.0 or -1.0,\n","gan_optimizer = tf.keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0)\n","gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"]},{"cell_type":"markdown","metadata":{"id":"CKfXBFMbOr8R"},"source":["### Train the GAN\n","\n","Now we can start training. To recapitulate, this is schematically what the training loop looks like:\n","\n","for each epoch:\n","- Draw random points in the latent space (random noise).\n","- Generate images with `generator` using this random noise.\n","- Mix the generated images with real ones.\n","- Train `discriminator` using these mixed images, with corresponding targets, either \"real\" (for the real images) or \"fake\" (for the generated images).\n","- Draw new random points in the latent space.\n","- Train `gan` using these random vectors, with targets that all say \"these are real images\". This will update the weights of the generator (only, since discriminator is frozen inside `gan`) to move them towards getting the discriminator to predict \"these are real images\" for generated images, i.e. this trains the generator to fool the discriminator."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"xFI2-0MaOr8S","scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 2s 2s/step\n","discriminator loss at step 0: 1.8701902627944946\n","adversarial loss at step 0: 0.002916492987424135\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 4s 4s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 4s 4s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","discriminator loss at step 100: 0.2657155990600586\n","adversarial loss at step 100: 0.0002861660614144057\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","discriminator loss at step 200: -162.6790313720703\n","adversarial loss at step 200: 13533.224609375\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 4s 4s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","discriminator loss at step 300: 3.6703543663024902\n","adversarial loss at step 300: 78.93839263916016\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 3s 3s/step\n"]}],"source":["import os\n","from tensorflow.keras.preprocessing import image\n","\n","# Load CIFAR10 data\n","(x_train, y_train), (_, _) = tf.keras.datasets.cifar10.load_data()\n","\n","# Select frog images (class 6)\n","x_train = x_train[y_train.flatten() == 6]\n","\n","# Normalize data\n","x_train = x_train.reshape(\n","    (x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n","\n","iterations = 2000\n","batch_size = 20\n","#save_dir = './results/gan' # make folder\n","save_dir='/content/Gan'\n","# Start training loop\n","start = 0\n","for step in range(iterations):\n","    # Sample random points in the latent space\n","    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n","\n","    # Decode them to fake images\n","    generated_images = generator.predict(random_latent_vectors)\n","\n","    # Combine them with real images\n","    stop = start + batch_size\n","    real_images = x_train[start: stop]\n","    combined_images = np.concatenate([generated_images, real_images])\n","\n","    # Assemble labels discriminating real from fake images\n","    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n","    # Add random noise to the labels - important trick!\n","    labels += 0.05 * np.random.random(labels.shape) #+= ex x+=5  means  x=x+5\n","\n","    # Train the discriminator\n","    d_loss = discriminator.train_on_batch(combined_images, labels)\n","\n","    # sample random points in the latent space\n","    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n","\n","    # Assemble labels that say \"all real images\"\n","    misleading_targets = np.zeros((batch_size, 1))\n","\n","    # Train the generator (via the gan model,\n","    # where the discriminator weights are frozen)\n","    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n","\n","    start += batch_size\n","    if start \u003e len(x_train) - batch_size:\n","        start = 0\n","\n","    # Occasionally save / plot\n","    if step % 100 == 0:  # after each 100 steps\n","        # Save model weights\n","        gan.save_weights('/content/Gan/gan.h5')\n","\n","        # Print metrics\n","        print('discriminator loss at step %s: %s' % (step, d_loss))\n","        print('adversarial loss at step %s: %s' % (step, a_loss))\n","\n","        # Save one generated image\n","        img = image.array_to_img(generated_images[0] * 255., scale=False)\n","        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n","\n","        # Save one real image, for comparison\n","        img = image.array_to_img(real_images[0] * 255., scale=False)\n","        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))"]},{"cell_type":"markdown","metadata":{"id":"fByFgoaJOr8X"},"source":["Let's display a few of our fake images:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1707192597645,"user":{"displayName":"kumod kumar gupta","userId":"07478795497397479293"},"user_tz":-330},"id":"uctCDl8XOr8Y","scrolled":false},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Sample random points in the latent space\n","random_latent_vectors = np.random.normal(size=(10, latent_dim))\n","\n","# Decode them to fake images\n","generated_images = generator.predict(random_latent_vectors)\n","\n","for i in range(generated_images.shape[0]):\n","    img = image.array_to_img(generated_images[i] * 255., scale=False)\n","    plt.figure()\n","    plt.imshow(img)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1707192597646,"user":{"displayName":"kumod kumar gupta","userId":"07478795497397479293"},"user_tz":-330},"id":"YKTO9yAmOr8b","scrolled":false},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3.6.10 64-bit ('dl-python': conda)","language":"python","name":"python361064bitdlpythoncondad344b632c0d143ada03beb1bf0544059"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"nbformat":4,"nbformat_minor":0}